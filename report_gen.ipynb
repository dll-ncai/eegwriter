{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cereprocess.datasets.pipeline import general_pipeline, neurotransformer_pipeline, resample\n",
    "from cereprocess.datasets.channels import NEUROTRANSFORMER_CHANNELS\n",
    "from models.neurogate import NeuroGate\n",
    "from models.neurotransformer import Neurotransformer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import ollama\n",
    "import gc\n",
    "from pdr import PDREstimator\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "CHANNEL_REGIONS = {\n",
    "    \"Frontal\": [\"FP1\", \"FP2\", \"F3\", \"F4\", \"FZ\"],\n",
    "    \"Left Temporal\": [\"F7\", \"T3\", \"T5\"],\n",
    "    \"Right Temporal\": [\"F8\", \"T4\", \"T6\"],\n",
    "    \"Central\": [\"C3\", \"C4\", \"CZ\"],\n",
    "    \"Parietal\": [\"P3\", \"P4\", \"PZ\"],\n",
    "    \"Occipital\": [\"O1\", \"O2\"]\n",
    "}\n",
    "\n",
    "# These models must be pulled using ollama\n",
    "MODELS = [\n",
    "    \"llama3.1\",\n",
    "    \"qwen3:8b\", \n",
    "    \"gemma3:12b\", \n",
    "    \"mistral-nemo:12b\", \n",
    "    \"deepseek-r1:8b\",\n",
    "    \"glm4:9b\",\n",
    "    \"koesn/llama3-openbiollm-8b\", \n",
    "    \"meditron:7b\", \n",
    "    ]\n",
    "\n",
    "# The .edf files must have same name as .txt files in reports\n",
    "# The files must be separated into two subfolders: normal and abnormal\n",
    "EDF_ROOT = \"path/to/edf/files/\"\n",
    "GROUND_TRUTH_DIR = \"reports_extracted_txt\" \n",
    "REPORTS = f\"reports_gen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(dir):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(dir, 'normal'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(dir, 'abnormal'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "neurogate = NeuroGate().to(device)\n",
    "neurotransformer = Neurotransformer().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "neurogate.load_state_dict(torch.load('model_weights/neurogate_wgts.pt'))\n",
    "neurogate.eval()\n",
    "neurotransformer.load_state_dict(torch.load('model_weights/neurotransformer_wgts.pth'))\n",
    "neurotransformer.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pipelines\n",
    "neurogate_pl = general_pipeline('NMT')\n",
    "neurotransformer_pl = neurotransformer_pipeline('NMT')\n",
    "pdr_pl = resample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define PDREstimator\n",
    "o1_idx = NEUROTRANSFORMER_CHANNELS.index(\"O1\")\n",
    "o2_idx = NEUROTRANSFORMER_CHANNELS.index(\"O2\")\n",
    "estimator = PDREstimator(200, o1_idx, o2_idx, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ab_prob(mne_data):\n",
    "    # Pass through neurogate\n",
    "    processed_data = neurogate_pl.apply(mne_data)\n",
    "    data = processed_data.get_data()\n",
    "    data = data[None, :, :]  \n",
    "    data = torch.from_numpy(data).float().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = neurogate(data)\n",
    "\n",
    "    ab_prob = list(F.softmax(outputs).cpu().numpy().reshape(-1,))[1] * 100\n",
    "    return ab_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clinical_adjective(percentage, threshold):\n",
    "    \"\"\"\n",
    "    Returns clinical quantification terms based on ACNS 2021 Guidelines.\n",
    "    Reference: Hirsch LJ, et al. J Clin Neurophysiol. 2021.\n",
    "    \"\"\"\n",
    "    if percentage < 1.0:\n",
    "        return \"Rare\"\n",
    "    elif percentage < threshold + 10.0:\n",
    "        return \"Occasional\"  # Shifted up (was 1-10%)\n",
    "    elif percentage < 50.0:\n",
    "        return \"Frequent\"    # (15-49%)\n",
    "    elif percentage < 90.0:\n",
    "        return \"Abundant\"    # (50-89%)\n",
    "    else:\n",
    "        return \"Continuous\"  # (>= 90%)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region_report(mne_data, threshold):\n",
    "    all_events = np.array([\"normal wave\", \"spike and sharp wave\", \"slow wave\"])\n",
    "\n",
    "    processed_data = neurotransformer_pl.apply(mne_data)\n",
    "    data = processed_data.get_data()\n",
    "\n",
    "    result_events = {}\n",
    "\n",
    "    for i, ch_name in enumerate(NEUROTRANSFORMER_CHANNELS):\n",
    "        ch_data = data[:, i:i+1, :]\n",
    "        ch_data = torch.from_numpy(ch_data).float().to(device)\n",
    "        outputs=None\n",
    "        with torch.no_grad():\n",
    "            outputs = neurotransformer(ch_data)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        outputs = outputs.argmax(axis=1)\n",
    "        events = list(all_events[outputs])\n",
    "        result_events[ch_name] = events\n",
    "\n",
    "    region_report = {}\n",
    "\n",
    "    for region_name, channels in CHANNEL_REGIONS.items():\n",
    "        total_windows = 0\n",
    "        spike_count = 0\n",
    "        slow_count = 0\n",
    "        \n",
    "        for ch in channels:\n",
    "            events = result_events[ch]\n",
    "            total_windows += len(events)\n",
    "            spike_count += events.count(\"spike and sharp wave\")\n",
    "            slow_count += events.count(\"slow wave\")\n",
    "\n",
    "        # Calculate Percentages\n",
    "        spike_pct = (spike_count / total_windows) * 100\n",
    "        slow_pct = (slow_count / total_windows) * 100\n",
    "        \n",
    "        # Generate Clinical Descriptors\n",
    "        findings = []\n",
    "\n",
    "        if spike_pct >= threshold: # Threshold to report it\n",
    "            adj = get_clinical_adjective(spike_pct, threshold)\n",
    "            findings.append(f\"{adj} epileptiform discharges\")\n",
    "            \n",
    "        if slow_pct >= threshold:\n",
    "            adj = get_clinical_adjective(slow_pct, threshold)\n",
    "            findings.append(f\"{adj} slowing\")\n",
    "\n",
    "        if not findings:\n",
    "            description = \"Normal activity.\"\n",
    "        else:\n",
    "            description = \", \".join(findings) + \".\"\n",
    "\n",
    "        region_report[region_name] = {\n",
    "            \"description\": description,\n",
    "            \"stats\": {\n",
    "                \"spike_pct\": spike_pct,\n",
    "                \"slow_pct\": slow_pct\n",
    "            }\n",
    "        }\n",
    "    return region_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdr(mne_data):\n",
    "    # Get PDR\n",
    "    processed_data = pdr_pl.apply(mne_data)\n",
    "    data = processed_data.get_data()\n",
    "    pdr_res = estimator.fit(data)\n",
    "\n",
    "    # Format PDR for the report\n",
    "    if pdr_res['pdr_o1'] and pdr_res['pdr_o2']:\n",
    "        avg_pdr = (pdr_res['pdr_o1'] + pdr_res['pdr_o2']) / 2\n",
    "        pdr_text = f\"{avg_pdr:.1f} Hz\"\n",
    "    elif pdr_res['pdr_o1'] or pdr_res['pdr_o2']:\n",
    "        val = pdr_res['pdr_o1'] or pdr_res['pdr_o2']\n",
    "        pdr_text = f\"{val:.1f} Hz\"\n",
    "    else:\n",
    "        pdr_text = \"Not well-formed\"\n",
    "\n",
    "    return pdr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on each file for each model\n",
    "sub_folders = ['normal', 'abnormal']\n",
    "\n",
    "for model in MODELS:\n",
    "    for folder in sub_folders:\n",
    "        dir = REPORTS + f\"_{model}\"\n",
    "        ensure_dir(dir)\n",
    "        folder_path = os.path.join(EDF_ROOT, folder)\n",
    "\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            if filename.replace('.edf', '.txt') in os.listdir(os.path.join(dir, folder)):\n",
    "                print(f\"Report already exists for {filename}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mne_data = mne.io.read_raw_edf(file_path, preload=True, verbose=False)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "            ab_prob = get_ab_prob(mne_data)\n",
    "            # print(ab_prob)\n",
    "\n",
    "            region_report = get_region_report(mne_data, 25)\n",
    "            # for region in CHANNEL_REGIONS.keys():\n",
    "                # print(region_report[region])\n",
    "\n",
    "            pdr_text = get_pdr(mne_data)\n",
    "            # print(pdr_text)\n",
    "\n",
    "            # Construct the Prompt\n",
    "            prompt_content = f\"\"\"\n",
    "            PATIENT STATISTICS:\n",
    "            - Global Abnormality Probability: {ab_prob:.1f}% (If >50%, consider Abnormal)\n",
    "            - Posterior Dominant Rhythm: {pdr_text}\n",
    "            \n",
    "            REGIONAL ANALYSIS:\n",
    "            \"\"\"\n",
    "            for region, desc in region_report.items():\n",
    "                prompt_content += f\"- {region}: {desc}\\n\"\n",
    "\n",
    "            system_instruction = \"\"\"\n",
    "            You are a clinical neurologist. Write a standard EEG report based strictly on the provided statistics.\n",
    "            \n",
    "            Format Requirements:\n",
    "            1. Use exactly two sections: \"FACTUAL REPORT\" and \"IMPRESSION\". It should not have any other sections.\n",
    "            2. In FACTUAL REPORT, describe the background (PDR) first, then regional findings.\n",
    "            3. In IMPRESSION, state \"Normal EEG\" or \"Abnormal EEG\" followed by a summary sentence.\n",
    "            4. Absolutely do not mention percentages in the final text; use clinical terms (Frequent, Occasional).\n",
    "            5. Write each section as a paragraph.\n",
    "            6. Do not use points.\n",
    "            7. Do not describe each and every region separately.\n",
    "            8. Do not assume any information on your own.\n",
    "\n",
    "            EXAMPLES:\n",
    "            Normal Report:\n",
    "            FACTUAL REPORT: Background rhythm shows alpha waveform seen around 8 Hz which is appropriate for age. Photic stimulation and HV not performed due to the state of patient. Intermittent EMG artifacts were seen. Stage II sleep was not achieved.\n",
    "\n",
    "            IMPRESSION: This EEG showed very mild encephalopathy and there is no element of non convulsive status. Kindly correlate with clinical picture.\n",
    "\n",
    "            Abnormal Report:\n",
    "            FACTUAL REPORT: Background rhythm during awake stage shows well-organized, welldeveloped, average voltage 10 hertz alpha activity in the posterior regions which is appropriate for age. It blocks with eye opening and it is bilaterally synchronous and symmetrical. Beta activity in the frontal or central areas is seen with average voltage and amplitude. Photic stimulation and hyperventilation was performed. Intermittent EMG artifacts were seen. Stage II sleep was not achieved.\n",
    "\n",
    "            IMPRESSION: This is an abnormal EEG with asymmetrical features there is delta wave slowing from right hemisphere. There are some faster frequencies on left side also that could be due to breach rhythm .kindly correlate clinically\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                response = ollama.chat(model=model, messages=[\n",
    "                    {'role': 'system', 'content': system_instruction},\n",
    "                    {'role': 'user', 'content': prompt_content},\n",
    "                ])\n",
    "                \n",
    "                report_text = response['message']['content']\n",
    "                \n",
    "                # Save to text file\n",
    "                out_file = os.path.join(dir, folder, filename.replace('.edf', '.txt'))\n",
    "                with open(out_file, \"w\") as f:\n",
    "                    f.write(report_text)\n",
    "                    \n",
    "                print(f\"Saved report to: {out_file}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Ollama Error: {e}\")\n",
    "\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data for all models...\n",
      "  -> llama3.1: Loaded 676 pairs.\n",
      "  -> qwen3:8b: Loaded 676 pairs.\n",
      "  -> gemma3:12b: Loaded 676 pairs.\n",
      "  -> mistral-nemo:12b: Loaded 676 pairs.\n",
      "  -> deepseek-r1:8b: Loaded 676 pairs.\n",
      "  -> glm4:9b: Loaded 676 pairs.\n",
      "  -> koesn/llama3-openbiollm-8b: Loaded 676 pairs.\n",
      "  -> meditron:7b: Loaded 676 pairs.\n",
      "\n",
      "--- Computing Standard Metrics (BLEU, ROUGE, METEOR) ---\n",
      "llama3.1: BLEU=0.09, ROUGE=0.23, METEOR=0.26\n",
      "qwen3:8b: BLEU=0.10, ROUGE=0.25, METEOR=0.27\n",
      "gemma3:12b: BLEU=0.02, ROUGE=0.18, METEOR=0.19\n",
      "mistral-nemo:12b: BLEU=0.09, ROUGE=0.25, METEOR=0.30\n",
      "deepseek-r1:8b: BLEU=0.06, ROUGE=0.21, METEOR=0.23\n",
      "glm4:9b: BLEU=0.03, ROUGE=0.17, METEOR=0.27\n",
      "koesn/llama3-openbiollm-8b: BLEU=0.02, ROUGE=0.10, METEOR=0.10\n",
      "meditron:7b: BLEU=0.00, ROUGE=0.07, METEOR=0.10\n",
      "\n",
      "--- Loading DistilBERT (Batch: 64) ... ---\n",
      "   -> Processing llama3.1... F1: 0.8099\n",
      "   -> Processing qwen3:8b... F1: 0.8172\n",
      "   -> Processing gemma3:12b... F1: 0.7870\n",
      "   -> Processing mistral-nemo:12b... F1: 0.8104\n",
      "   -> Processing deepseek-r1:8b... F1: 0.7981\n",
      "   -> Processing glm4:9b... F1: 0.7811\n",
      "   -> Processing koesn/llama3-openbiollm-8b... F1: 0.7390\n",
      "   -> Processing meditron:7b... F1: 0.6705\n",
      "   -> Unloading DistilBERT...\n",
      "\n",
      "--- Loading RoBERTa (Batch: 32) ... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> Processing llama3.1... F1: 0.8667\n",
      "   -> Processing qwen3:8b... F1: 0.8701\n",
      "   -> Processing gemma3:12b... F1: 0.8499\n",
      "   -> Processing mistral-nemo:12b... F1: 0.8641\n",
      "   -> Processing deepseek-r1:8b... F1: 0.8653\n",
      "   -> Processing glm4:9b... F1: 0.8503\n",
      "   -> Processing koesn/llama3-openbiollm-8b... F1: 0.8270\n",
      "   -> Processing meditron:7b... F1: 0.7606\n",
      "   -> Unloading RoBERTa...\n",
      "\n",
      "--- Loading DeBERTa (Batch: 2) ... ---\n",
      "   -> Processing llama3.1... F1: 0.6263\n",
      "   -> Processing qwen3:8b... F1: 0.6465\n",
      "   -> Processing gemma3:12b... F1: 0.5964\n",
      "   -> Processing mistral-nemo:12b... F1: 0.6386\n",
      "   -> Processing deepseek-r1:8b... F1: 0.6228\n",
      "   -> Processing glm4:9b... F1: 0.5917\n",
      "   -> Processing koesn/llama3-openbiollm-8b... F1: 0.5109\n",
      "   -> Processing meditron:7b... F1: 0.4297\n",
      "   -> Unloading DeBERTa...\n",
      "\n",
      "================================================================================================\n",
      "Model Name                     BLEU       ROUGE      METEOR     Distil     RoBERTa    DeBERTa   \n",
      "------------------------------------------------------------------------------------------------\n",
      "llama3.1                       0.0861     0.2294     0.2646     0.8099     0.8667     0.6263    \n",
      "qwen3:8b                       0.0990     0.2457     0.2653     0.8172     0.8701     0.6465    \n",
      "gemma3:12b                     0.0169     0.1817     0.1880     0.7870     0.8499     0.5964    \n",
      "mistral-nemo:12b               0.0898     0.2529     0.3033     0.8104     0.8641     0.6386    \n",
      "deepseek-r1:8b                 0.0626     0.2069     0.2304     0.7981     0.8653     0.6228    \n",
      "glm4:9b                        0.0302     0.1691     0.2729     0.7811     0.8503     0.5917    \n",
      "koesn/llama3-openbiollm-8b     0.0175     0.0987     0.0954     0.7390     0.8270     0.5109    \n",
      "meditron:7b                    0.0048     0.0653     0.0999     0.6705     0.7606     0.4297    \n",
      "================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from tqdm.notebook import tqdm\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Data loading function\n",
    "def load_data_for_all_models(gt_dir):\n",
    "    \"\"\"\n",
    "    Pre-loads all text data so we don't have to keep reading files.\n",
    "    Returns a dictionary: { model_key: {'refs': [], 'cands': []} }\n",
    "    \"\"\"\n",
    "    all_data = {}\n",
    "    \n",
    "    print(\"Loading text data for all models...\")\n",
    "    for model_key in MODELS:\n",
    "        gen_dir = f\"reports_gen_{model_key}\"\n",
    "        \n",
    "        refs = []\n",
    "        cands = []\n",
    "        \n",
    "        if not os.path.exists(gen_dir):\n",
    "            print(f\"Skipping {model_key} (Directory not found: {gen_dir})\")\n",
    "            continue\n",
    "            \n",
    "        for root, dirs, files in os.walk(gen_dir):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".txt\"): continue\n",
    "                \n",
    "                gen_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(gen_path, gen_dir)\n",
    "                gt_path = os.path.join(gt_dir, rel_path)\n",
    "                \n",
    "                if os.path.exists(gt_path):\n",
    "                    with open(gen_path, 'r', encoding='utf-8') as f:\n",
    "                        cands.append(f.read().strip())\n",
    "                    with open(gt_path, 'r', encoding='utf-8') as f:\n",
    "                        refs.append(f.read().strip())\n",
    "        \n",
    "        all_data[model_key] = {'refs': refs, 'cands': cands, 'name': model_key}\n",
    "        print(f\"  -> {model_key}: Loaded {len(refs)} pairs.\")\n",
    "        \n",
    "    return all_data\n",
    "\n",
    "# Metric Calculations\n",
    "def get_cpu_metrics(refs, cands):\n",
    "    \"\"\"Computes BLEU-4, ROUGE-L, and METEOR (No GPU needed)\"\"\"\n",
    "    # Tokenize for BLEU/METEOR\n",
    "    ref_tokens = [[nltk.word_tokenize(r)] for r in refs] \n",
    "    cand_tokens = [nltk.word_tokenize(c) for c in cands]\n",
    "    \n",
    "    # BLEU-4\n",
    "    bleu = corpus_bleu(ref_tokens, cand_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    \n",
    "    # METEOR\n",
    "    meteor_scores = [meteor_score(r, c) for r, c in zip(ref_tokens, cand_tokens)]\n",
    "    meteor = np.mean(meteor_scores)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_f1s = [scorer.score(r, c)['rougeL'].fmeasure for r, c in zip(refs, cands)]\n",
    "    rouge = np.mean(rouge_l_f1s)\n",
    "    \n",
    "    return bleu, rouge, meteor\n",
    "\n",
    "\n",
    "# Load all data\n",
    "model_data = load_data_for_all_models(GROUND_TRUTH_DIR)\n",
    "results = {key: {} for key in model_data}\n",
    "\n",
    "# 2. Compute CPU Metrics (BLEU, ROUGE, METEOR)\n",
    "print(\"\\n--- Computing Standard Metrics (BLEU, ROUGE, METEOR) ---\")\n",
    "for key, data in model_data.items():\n",
    "    if not data['refs']: continue\n",
    "    b, r, m = get_cpu_metrics(data['refs'], data['cands'])\n",
    "    results[key]['BLEU'] = b\n",
    "    results[key]['ROUGE'] = r\n",
    "    results[key]['METEOR'] = m\n",
    "    print(f\"{data['name']}: BLEU={b:.2f}, ROUGE={r:.2f}, METEOR={m:.2f}\")\n",
    "\n",
    "# Compute BERTScore\n",
    "# We load one BERT model, process ALL LLMs, then unload it.\n",
    "bert_models = [\n",
    "    (\"distilbert-base-uncased\", \"DistilBERT\", 64),\n",
    "    (\"roberta-large\", \"RoBERTa\", 32),\n",
    "    (\"microsoft/deberta-xlarge-mnli\", \"DeBERTa\", 2) \n",
    "]\n",
    "\n",
    "# Safety limit: Truncate text to ~3000 chars\n",
    "MAX_CHAR_LENGTH = 3000 \n",
    "\n",
    "for model_path, short_name, batch_size in bert_models:\n",
    "    print(f\"\\n--- Loading {short_name} (Batch: {batch_size}) ... ---\")\n",
    "    \n",
    "    # Load model\n",
    "    scorer = BERTScorer(model_type=model_path, lang=\"en\", rescale_with_baseline=False, device='cuda')\n",
    "    \n",
    "    for key, data in model_data.items():\n",
    "        if not data['refs']: continue\n",
    "        \n",
    "        # Truncate long strings \n",
    "        # Replace empty strings \n",
    "        clean_cands = []\n",
    "        for c in data['cands']:\n",
    "            c_trunc = c[:MAX_CHAR_LENGTH]\n",
    "            # Placeholder for empty files\n",
    "            if not c_trunc.strip():\n",
    "                c_trunc = \".\" \n",
    "            clean_cands.append(c_trunc)\n",
    "            \n",
    "        clean_refs = [r[:MAX_CHAR_LENGTH] for r in data['refs']]\n",
    "\n",
    "        print(f\"   -> Processing {data['name']}...\", end=\" \", flush=True)\n",
    "        \n",
    "        # Pass the cleaned lists\n",
    "        P, R, F1 = scorer.score(clean_cands, clean_refs, batch_size=batch_size, verbose=False)\n",
    "        \n",
    "        results[key][short_name] = F1.mean().item()\n",
    "        print(f\"F1: {F1.mean().item():.4f}\")\n",
    "        \n",
    "        del P, R, F1, clean_cands, clean_refs\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"   -> Unloading {short_name}...\")\n",
    "    del scorer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "# Header formatting\n",
    "header = f\"{'Model Name':<30} {'BLEU':<10} {'ROUGE':<10} {'METEOR':<10} {'Distil':<10} {'RoBERTa':<10} {'DeBERTa':<10}\"\n",
    "print(\"\\n\" + \"=\"*len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for key in MODELS:\n",
    "    if key not in results or not results[key]:\n",
    "        # Handle case where model wasn't found/processed\n",
    "        print(f\"{key:<30} -- Data not found --\")\n",
    "    else:\n",
    "        r = results[key]\n",
    "        name = model_data[key]['name']\n",
    "        \n",
    "        # Formatted row with fixed width spacing\n",
    "        row_str = (f\"{name:<30} \"\n",
    "                   f\"{r.get('BLEU', 0):<10.4f} \"\n",
    "                   f\"{r.get('ROUGE', 0):<10.4f} \"\n",
    "                   f\"{r.get('METEOR', 0):<10.4f} \"\n",
    "                   f\"{r.get('DistilBERT', 0):<10.4f} \"\n",
    "                   f\"{r.get('RoBERTa', 0):<10.4f} \"\n",
    "                   f\"{r.get('DeBERTa', 0):<10.4f}\")\n",
    "        print(row_str)\n",
    "\n",
    "print(\"=\"*len(header) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braindecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
